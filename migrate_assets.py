#!/usr/bin/env python3
"""
è¿ç§»è„šæœ¬ï¼šå°†åšå®¢ä¸­çš„å›¾ç‰‡å’Œè§†é¢‘ä»æ—§å­˜å‚¨è¿ç§»åˆ°æ–°çš„S3å…¼å®¹æœåŠ¡
- æ—§MinIO: https://minio.menghuan1918.com:443/markdown
- æ—§Alist: https://blog.menghuan1918.com/AlistStore/
- æ–°S3: https://rustfs.menghuan1918.com/markdown
"""

import re
import subprocess
import tempfile
import hashlib
from pathlib import Path
from urllib.parse import unquote, urlparse
from typing import NamedTuple

class UrlMapping(NamedTuple):
    """URLæ˜ å°„è®°å½•"""
    old_url: str
    new_url: str
    file_path: str  # mdæ–‡ä»¶è·¯å¾„
    download_url: str  # å®é™…ä¸‹è½½ç”¨çš„URL
    s3_key: str  # S3ä¸­çš„key


# é…ç½®
OLD_MINIO_BASE = "https://minio.menghuan1918.com:443/markdown"
OLD_ALIST_BASE = "https://blog.menghuan1918.com/AlistStore"
NEW_S3_BASE = "https://rustfs.menghuan1918.com/markdown"
MC_ALIAS = "rustfs"
MC_BUCKET = "markdown"
MCLI = "mcli"

# æ‰«æç›®å½•
SCAN_DIRS = ["src/posts", "src/en/posts"]

# æ­£åˆ™è¡¨è¾¾å¼
MINIO_PATTERN = re.compile(
    r'https://minio\.menghuan1918\.com:443/markdown/[^\s\)"\'<>]+'
)
ALIST_PATTERN = re.compile(
    r'https://blog\.menghuan1918\.com/AlistStore/[^\s\)"\'<>]+'
)


def get_project_root() -> Path:
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    return Path(__file__).parent


def extract_urls_from_file(file_path: Path) -> list[tuple[str, str]]:
    """ä»markdownæ–‡ä»¶ä¸­æå–éœ€è¦è¿ç§»çš„URL

    Returns:
        list of (url, url_type) where url_type is 'minio' or 'alist'
    """
    urls = []
    content = file_path.read_text(encoding="utf-8")

    # æŸ¥æ‰¾MinIO URLs
    for match in MINIO_PATTERN.finditer(content):
        urls.append((match.group(0), "minio"))

    # æŸ¥æ‰¾Alist URLs
    for match in ALIST_PATTERN.finditer(content):
        urls.append((match.group(0), "alist"))

    return urls


def normalize_alist_url(url: str) -> tuple[str, str]:
    """è§„èŒƒåŒ–Alist URLï¼Œè¿”å› (ä¸‹è½½URL, æ–°çš„S3è·¯å¾„)

    Alist URLæœ‰å¤šç§æ ¼å¼ï¼š
    1. https://blog.menghuan1918.com/AlistStore/xxx
    2. https://blog.menghuan1918.com/AlistStore/d/opt/alist/data/store/opt/alist/data/store/xxx?sign=...
    """
    parsed = urlparse(url)
    path = unquote(parsed.path)

    # å»æ‰ç­¾åå‚æ•°ç”¨äºç”Ÿæˆæ–°è·¯å¾„
    clean_path = path

    # å¤„ç†å¸¦/d/çš„è·¯å¾„ - è¿™æ˜¯Alistçš„ä¸‹è½½è·¯å¾„æ ¼å¼
    if "/AlistStore/d/" in path:
        # æå–å®é™…æ–‡ä»¶è·¯å¾„ï¼š/opt/alist/data/store/opt/alist/data/store/xxx -> xxx
        # è·¯å¾„æ ¼å¼: /AlistStore/d/opt/alist/data/store/opt/alist/data/store/å®é™…è·¯å¾„
        match = re.search(r'/AlistStore/d/opt/alist/data/store/opt/alist/data/store/(.+)$', path)
        if match:
            actual_path = match.group(1)
            clean_path = f"/AlistStore/{actual_path}"

    # ç”ŸæˆS3 keyï¼šå»æ‰ /AlistStore å‰ç¼€ï¼Œæ”¾åˆ° alist å­ç›®å½•
    s3_path = clean_path.replace("/AlistStore/", "alist/")

    # ä¸‹è½½URLï¼šå¯¹äºå¸¦ç­¾åçš„ä¿ç•™ç­¾åï¼Œå¦åˆ™ä½¿ç”¨åŸURL
    download_url = url

    return download_url, s3_path


def normalize_minio_url(url: str) -> tuple[str, str]:
    """è§„èŒƒåŒ–MinIO URLï¼Œè¿”å› (ä¸‹è½½URL, æ–°çš„S3è·¯å¾„)"""
    parsed = urlparse(url)
    path = unquote(parsed.path)

    # ç§»é™¤ /markdown å‰ç¼€
    s3_path = path.replace("/markdown/", "", 1)

    return url, s3_path


def generate_new_url(s3_path: str) -> str:
    """ç”Ÿæˆæ–°çš„URL"""
    return f"{NEW_S3_BASE}/{s3_path}"


def download_file(url: str, dest_path: Path) -> bool:
    """ä¸‹è½½æ–‡ä»¶"""
    print(f"  ä¸‹è½½: {url[:80]}...")
    try:
        result = subprocess.run(
            ["curl", "-fsSL", "-o", str(dest_path), url],
            capture_output=True,
            text=True,
            timeout=120
        )
        if result.returncode != 0:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {result.stderr}")
            return False
        return True
    except subprocess.TimeoutExpired:
        print("  âŒ ä¸‹è½½è¶…æ—¶")
        return False
    except Exception as e:
        print(f"  âŒ ä¸‹è½½å¼‚å¸¸: {e}")
        return False


def upload_to_s3(local_path: Path, s3_key: str) -> bool:
    """ä½¿ç”¨mcä¸Šä¼ åˆ°S3"""
    dest = f"{MC_ALIAS}/{MC_BUCKET}/{s3_key}"
    print(f"  ä¸Šä¼ : {dest}")
    try:
        result = subprocess.run(
            [MCLI, "cp", str(local_path), dest],
            capture_output=True,
            text=True,
            timeout=120
        )
        if result.returncode != 0:
            print(f"  âŒ ä¸Šä¼ å¤±è´¥: {result.stderr}")
            return False
        return True
    except subprocess.TimeoutExpired:
        print("  âŒ ä¸Šä¼ è¶…æ—¶")
        return False
    except Exception as e:
        print(f"  âŒ ä¸Šä¼ å¼‚å¸¸: {e}")
        return False


def update_markdown_file(file_path: Path, old_url: str, new_url: str) -> bool:
    """æ›´æ–°markdownæ–‡ä»¶ä¸­çš„URL"""
    try:
        content = file_path.read_text(encoding="utf-8")
        # ä½¿ç”¨ re.escape ç¡®ä¿URLä¸­çš„ç‰¹æ®Šå­—ç¬¦è¢«æ­£ç¡®è½¬ä¹‰
        new_content = content.replace(old_url, new_url)
        if content != new_content:
            file_path.write_text(new_content, encoding="utf-8")
            return True
        return False
    except Exception as e:
        print(f"  âŒ æ›´æ–°æ–‡ä»¶å¤±è´¥: {e}")
        return False


def collect_all_urls() -> dict[str, list[UrlMapping]]:
    """æ”¶é›†æ‰€æœ‰éœ€è¦è¿ç§»çš„URL

    Returns:
        dict: {download_url: [UrlMapping, ...]} ç”¨äºå»é‡
    """
    root = get_project_root()
    url_mappings: dict[str, list[UrlMapping]] = {}

    for scan_dir in SCAN_DIRS:
        dir_path = root / scan_dir
        if not dir_path.exists():
            print(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {scan_dir}")
            continue

        for md_file in dir_path.glob("*.md"):
            urls = extract_urls_from_file(md_file)
            for url, url_type in urls:
                if url_type == "minio":
                    download_url, s3_key = normalize_minio_url(url)
                else:
                    download_url, s3_key = normalize_alist_url(url)

                new_url = generate_new_url(s3_key)
                mapping = UrlMapping(
                    old_url=url,
                    new_url=new_url,
                    file_path=str(md_file),
                    download_url=download_url,
                    s3_key=s3_key
                )

                # ä½¿ç”¨ä¸‹è½½URLä½œä¸ºkeyè¿›è¡Œå»é‡ï¼ˆåŒä¸€æ–‡ä»¶å¯èƒ½åœ¨å¤šç¯‡æ–‡ç« ä¸­å¼•ç”¨ï¼‰
                if download_url not in url_mappings:
                    url_mappings[download_url] = []
                url_mappings[download_url].append(mapping)

    return url_mappings


def main():
    print("=" * 60)
    print("åšå®¢èµ„æºè¿ç§»è„šæœ¬")
    print(f"ä»: {OLD_MINIO_BASE}")
    print(f"    {OLD_ALIST_BASE}")
    print(f"åˆ°: {NEW_S3_BASE}")
    print("=" * 60)

    # æ”¶é›†æ‰€æœ‰URL
    print("\nğŸ“‹ æ­£åœ¨æ‰«æmarkdownæ–‡ä»¶...")
    url_mappings = collect_all_urls()

    if not url_mappings:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°éœ€è¦è¿ç§»çš„èµ„æº")
        return

    # ç»Ÿè®¡
    total_files = len(url_mappings)
    total_refs = sum(len(mappings) for mappings in url_mappings.values())
    print(f"\nğŸ“Š æ‰¾åˆ° {total_files} ä¸ªå”¯ä¸€èµ„æºæ–‡ä»¶ï¼Œå…± {total_refs} ä¸ªå¼•ç”¨")

    # æ˜¾ç¤ºé¢„è§ˆ
    print("\nğŸ“ é¢„è§ˆ (å‰10ä¸ª):")
    for i, (download_url, mappings) in enumerate(url_mappings.items()):
        if i >= 10:
            print(f"   ... è¿˜æœ‰ {len(url_mappings) - 10} ä¸ªæ–‡ä»¶")
            break
        m = mappings[0]
        print(f"   {m.s3_key}")
        print(f"      å¼•ç”¨: {len(mappings)} å¤„")

    # ç¡®è®¤
    print("\n" + "=" * 60)
    confirm = input("æ˜¯å¦å¼€å§‹è¿ç§»ï¼Ÿ[y/N]: ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    # å¼€å§‹è¿ç§»
    success_count = 0
    fail_count = 0

    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)

        for download_url, mappings in url_mappings.items():
            m = mappings[0]
            print(f"\nğŸ”„ å¤„ç†: {m.s3_key}")

            # ä¸‹è½½æ–‡ä»¶
            # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶å
            ext = Path(m.s3_key).suffix or ".bin"
            temp_file = temp_path / f"{hashlib.md5(download_url.encode()).hexdigest()}{ext}"

            if not download_file(m.download_url, temp_file):
                fail_count += 1
                continue

            # ä¸Šä¼ åˆ°S3
            if not upload_to_s3(temp_file, m.s3_key):
                fail_count += 1
                continue

            # æ›´æ–°æ‰€æœ‰å¼•ç”¨è¯¥èµ„æºçš„markdownæ–‡ä»¶
            for mapping in mappings:
                file_path = Path(mapping.file_path)
                if update_markdown_file(file_path, mapping.old_url, mapping.new_url):
                    print(f"  âœï¸  æ›´æ–°: {file_path.name}")

            success_count += 1

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_file.unlink(missing_ok=True)

    # æ€»ç»“
    print("\n" + "=" * 60)
    print("âœ… è¿ç§»å®Œæˆ!")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±è´¥: {fail_count}")
    print("=" * 60)


if __name__ == "__main__":
    main()
